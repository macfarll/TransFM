{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "working_PRME.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeA_GaxmzhOe"
      },
      "source": [
        "Loading & Preprocessing MovieLens Dataset\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM0x24hYvZC4",
        "outputId": "e3c00917-3921-47a1-a82b-98316c50ea9e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Download MovieLens data.\n",
        "print(\"Downloading movielens data...\")\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\", \"movielens.zip\")\n",
        "zip_ref = zipfile.ZipFile('movielens.zip', \"r\")\n",
        "zip_ref.extractall()\n",
        "print(\"Done. Dataset contains:\")\n",
        "print(zip_ref.read('ml-100k/u.info'))\n",
        "\n",
        "# Load each data set (users, movies, and ratings).\n",
        "users_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
        "users = pd.read_csv('ml-100k/u.user', sep='|', names=users_cols, encoding='latin-1')\n",
        "\n",
        "ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
        "ratings = pd.read_csv('ml-100k/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')\n",
        "\n",
        "# The movies file contains a binary feature for each genre.\n",
        "genre_cols = [\n",
        "    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
        "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
        "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
        "]\n",
        "\n",
        "movies_cols = [\n",
        "    'movie_id', 'title', 'release_date', \"video_release_date\", \"imdb_url\"\n",
        "] + genre_cols\n",
        "\n",
        "movies = pd.read_csv('ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')\n",
        "\n",
        "# Since the ids start at 1, we shift them to start at 0.\n",
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: str(x-1))\n",
        "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: str(x-1))\n",
        "movies[\"year\"] = movies['release_date'].apply(lambda x: str(x).split('-')[-1])\n",
        "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: str(x-1))\n",
        "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: str(x-1))\n",
        "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n",
        "\n",
        "# Compute the number of movies to which a genre is assigned.\n",
        "genre_occurences = movies[genre_cols].sum().to_dict()\n",
        "\n",
        "# Since some movies can belong to more than one genre, we create different\n",
        "# 'genre' columns as follows:\n",
        "# - all_genres: all the active genres of the movie.\n",
        "# - genre: randomly sampled from the active genres.\n",
        "def mark_genres(movies, genres):\n",
        "  def get_random_genre(gs):\n",
        "    active = [genre for genre, g in zip(genres, gs) if g==1]\n",
        "    if len(active) == 0:\n",
        "      return 'Other'\n",
        "    return np.random.choice(active)\n",
        "  def get_all_genres(gs):\n",
        "    active = [genre for genre, g in zip(genres, gs) if g==1]\n",
        "    if len(active) == 0:\n",
        "      return 'Other'\n",
        "    return '-'.join(active)\n",
        "  movies['genre'] = [\n",
        "      get_random_genre(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
        "  movies['all_genres'] = [\n",
        "      get_all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
        "\n",
        "mark_genres(movies, genre_cols)\n",
        "\n",
        "# Create one merged DataFrame containing all the movielens data.\n",
        "movielens = ratings.merge(movies, on='movie_id').merge(users, on='user_id')\n",
        "\n",
        "# Utility to split the data into training and test sets.\n",
        "def split_dataframe(df, holdout_fraction=0.1):\n",
        "  \"\"\"Splits a DataFrame into training and test sets.\n",
        "  Args:\n",
        "    df: a dataframe.\n",
        "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
        "  Returns:\n",
        "    train: dataframe for training\n",
        "    test: dataframe for testing\n",
        "  \"\"\"\n",
        "  test = df.sample(frac=holdout_fraction, replace=False)\n",
        "  train = df[~df.index.isin(test.index)]\n",
        "  return train, test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading movielens data...\n",
            "Done. Dataset contains:\n",
            "b'943 users\\n1682 items\\n100000 ratings\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y11ia9oCNlXs"
      },
      "source": [
        "Data PreProcessing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z93CgTrmvXk5"
      },
      "source": [
        "#Restructure the movielens dataframe into three input formats expected by the recommender\n",
        "\n",
        "# ml_item_feat is a unique listing of each movie (item) and the features associated with the item. \n",
        "#    In this case all features are genres, but features can be any number\n",
        "ml_item_feat = movielens[['movie_id', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', \n",
        "       'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']].drop_duplicates()\n",
        "\n",
        "# Rename 'movie_id' to 'item_id', as the model expectes the primary key to be 'item_id'\n",
        "ml_item_feat.columns = ['item_id', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', \n",
        "       'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
        "\n",
        "# Create another data frame for user features, in this example only age is used by any number can be used\n",
        "ml_user_feat = movielens[['user_id', 'age']].drop_duplicates()\n",
        "\n",
        "# Create a short data frame that only contains the relationships between which users interacted with which item\n",
        "ml_short_df = movielens[['user_id', 'movie_id', 'rating', 'unix_timestamp']]\n",
        "\n",
        "# Rename to columns to show the expected inputs\n",
        "ml_short_df.columns = ['user_id', 'item_id', 'rating', 'time']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfxBsZiSNxZy"
      },
      "source": [
        "Model Arguments \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew8gTlAusZwr"
      },
      "source": [
        "# A class used to save all the arguments used for a model, with a function to log arguments used\n",
        "class DummyArgs(object):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # Core args\n",
        "        self.filename = ml_short_df #name of pandas df (preformatted)\n",
        "        self.model = \"PRME\" # Currently supports \"PRME\", \"TransFM\"\n",
        "        self.features = \"full_features\" # Currently only supports \"full_features\", TODO: add \"time\"\n",
        "        self.item_df = ml_item_feat # The item pandas df (already formatted)\n",
        "        self.user_df = ml_user_feat # Already formatted user data \n",
        "        self.user_region_df = \"none\" # A data frame that restricts items and users to specific regions\n",
        "\n",
        "        # Training args\n",
        "        self.min_epoch = 30 # Minimum number of epochs, model will not stop even if accuracy is decreasing\n",
        "        self.eval_freq = 10 # Frequency at which to evaluate model\n",
        "        self.max_iters = 100 # Epoch to end training, even if model is still improving\n",
        "        self.quit_delta = 40 # Number of iterations at which to quit if no improvement\n",
        "        self.num_train_samples = 3 # Number of negative samples to evaluate against to calc auc\n",
        "        self.num_val_samples = 10 # Number of negative samples to compare against positive sample in val\n",
        "        self.num_test_samples = 10 # Recommended to be same as val\n",
        "        self.val_set = 1 # 1 to include validation set, 0 to only use train/test (skipping val set is very prone to overfitting)\n",
        "        self.weighted_sampling = 0 #set negative samples to be weighted according to observations in train\n",
        "\n",
        "        # Model args\n",
        "        self.num_dims = 3 # Model dimensionality\n",
        "        self.linear_reg = 6.236 #L2 regularization: linear regularization\n",
        "        self.emb_reg = 10.54 #L2 regularization: embedding regularization\n",
        "        self.trans_reg = 3.135 #from first pass #L2 regularization: translation regularization\n",
        "        self.init_mean = 0.133 \n",
        "        self.starting_lr = 0.05 # Learning rate of model at epoch 1\n",
        "        self.lr_decay_factor = 1.33 # Decay factor for learning rate\n",
        "        self.lr_decay_freq = 700 # Frequency at which to decay learning rate\n",
        "        self.user_min = 2 # Minimum number of interactions for a user to be included in model\n",
        "        self.item_min = 3 # Minimum number of interactions for an item to be included in model\n",
        "        self.secondary_reg_scale = 1 # Scale loss inside sigmoid function, does nothing if set to 1\n",
        "\n",
        "        # Debug args\n",
        "        self.verbosity = 1 # 2 for maximum verbosity, 0 supressess all\n",
        "        self.random_seed = 1 # Set the random seed to the model, useful for debugging\n",
        "        self.log_cache = list() # Init empty log \n",
        "\n",
        "        # Deployment args\n",
        "        self.return_k_preds = 100 # Number of predictions to return per user\n",
        "        self.deploy_preds = 1 # 0 disables the generation of deployment predictions for better performance, 1 to enable\n",
        "         \n",
        "    # A function to store input text to log_cache as well as print if the input verbosity_max is greater than the \n",
        "    # verbosity of the \n",
        "    def logger(self, input_text, verbosity_max = 1):\n",
        "        self.log_cache.append(input_text)\n",
        "        if self.verbosity >= verbosity_max:\n",
        "            print(input_text)\n",
        "            \n",
        "# Default args, set as something other than a command line implementation\n",
        "args = DummyArgs()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OttaNA35OFdb"
      },
      "source": [
        "Dataset Object\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPiww09Jm_U9"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "import random\n",
        "\n",
        "# Class to represent a dataset\n",
        "class Dataset:\n",
        "    def __init__(self, args):\n",
        "        \n",
        "        df = args.filename\n",
        "        self.args = args\n",
        "        \n",
        "        # Use random seed so that future runs with the same params are deterministic \n",
        "        np.random.seed(self.args.random_seed)\n",
        "        \n",
        "        print('First pass')\n",
        "        print('\\tnum_users = ' + str(len(df['user_id'].unique())))\n",
        "        print('\\tnum_items = ' + str(len(df['item_id'].unique())))\n",
        "        print('\\tdf_shape  = ' + str(df.shape))\n",
        "        \n",
        "        user_counts = df['user_id'].value_counts()\n",
        "        print('Collected user counts...')\n",
        "        item_counts = df['item_id'].value_counts()\n",
        "        print('Collected item counts...')\n",
        "        \n",
        "        # Filter based on user and item counts\n",
        "        df = df[df.apply(lambda x: user_counts[x['user_id']] >= self.args.user_min, axis=1)]\n",
        "        print('User filtering done...')\n",
        "        df = df[df.apply(lambda x: item_counts[x['item_id']] >= self.args.item_min, axis=1)]\n",
        "        print('Item filtering done...')\n",
        "        \n",
        "        print('Second pass')\n",
        "        self.args.logger('\\tnum_users = ' + str(len(df['user_id'].unique())), 1)\n",
        "        self.args.logger('\\tnum_items = ' + str(len(df['item_id'].unique())), 1)\n",
        "        self.args.logger('\\tdf_shape  = ' + str(df.shape), 1)\n",
        "        \n",
        "        # If restricting users to certain regions, assign regional mapping dataframe to dataset\n",
        "        if type(self.args.user_region_df) == pd.DataFrame:\n",
        "            \n",
        "            # Assign both dataframes as attributes of dataset\n",
        "            self.user_region_df = self.args.user_region_df\n",
        "            self.item_region_df = self.args.item_df[['item_id', 'region_id']]\n",
        "            \n",
        "        # Either way, make sure 'region_id' is no longer in item_df and set as dataset attribute\n",
        "        self.item_df = self.args.item_df.drop(columns = ['region_id'], errors='ignore')\n",
        "        \n",
        "        # Original code normalized temporal values here\n",
        "        \n",
        "        print('Constructing datasets...')\n",
        "        training_set = defaultdict(list)\n",
        "        # Start counting users and items at 1 to facilitate sparse matrix computation\n",
        "        # NOTE: this means these dictionaries will NOT be 0 indexed, careful tracking \n",
        "        #    of 0-indexed series is needed for development  \n",
        "        num_users = 0\n",
        "        num_items = 0\n",
        "        item_to_idx = {}\n",
        "        user_to_idx = {}\n",
        "        idx_to_item = {}\n",
        "        idx_to_user = {}\n",
        "        \n",
        "        # Iterate through items, creating dense dicts for item and users\n",
        "        for row in df.itertuples():\n",
        "            \n",
        "            # New item\n",
        "            if row.item_id not in item_to_idx:\n",
        "                item_to_idx[row.item_id] = num_items\n",
        "                idx_to_item[num_items] = row.item_id\n",
        "                num_items += 1\n",
        "                \n",
        "            # New user\n",
        "            if row.user_id not in user_to_idx:\n",
        "                user_to_idx[row.user_id] = num_users\n",
        "                idx_to_user[num_users] = row.user_id\n",
        "                num_users += 1\n",
        "                \n",
        "            # Converts all ratings to positive implicit feedback\n",
        "            training_set[user_to_idx[row.user_id]].append(\n",
        "                    (item_to_idx[row.item_id], row.time))\n",
        "        \n",
        "        # Save item and use count as attributes of dataset\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        \n",
        "        # Sort training_set by time, so each users' observations are in order\n",
        "        for user in training_set:\n",
        "            training_set[user].sort(key=lambda x: x[1])\n",
        "        \n",
        "        # Create dictionaries for user to region and region to user\n",
        "        if type(self.args.user_region_df) == pd.DataFrame:\n",
        "            \n",
        "            # Define default values to fall back on for users without regions                \n",
        "            default_item_list = range(self.num_items)\n",
        "            default_num_items = self.num_items\n",
        "            \n",
        "            # Only include users with enough data to be included after prefiltering\n",
        "            active_user_mask = self.args.user_region_df['user_id'].isin(df['user_id'].unique())\n",
        "            user_region_df = self.args.user_region_df.loc[active_user_mask]\n",
        "            \n",
        "            # Start with empty dictionaries for region lookups\n",
        "            item_to_region = {}\n",
        "            region_to_item = {}\n",
        "            user_to_region_item_idx = {}\n",
        "            user_to_num_items = {}\n",
        "            region_str_to_items = {}\n",
        "            \n",
        "            # Iterate over tall input data where each row is a single item, region pair\n",
        "            for row in self.item_region_df.itertuples():\n",
        "                \n",
        "                # Building dict with item_id as key, and value is that item's region\n",
        "                item_to_region[row.item_id] = [int(row.region_id)]\n",
        "                \n",
        "                # Check if this item is being used in dataset (has adequate data)\n",
        "                if item_to_idx.get(row.item_id):\n",
        "                \n",
        "                    # Building dict with region_id as key, and value is list of items in region\n",
        "                    if row.region_id not in region_to_item:\n",
        "\n",
        "                        # If this row is a new region then add entry to dict, where value is list with 1 value\n",
        "                        region_to_item[row.region_id] = [item_to_idx.get(row.item_id)] \n",
        "                    else:\n",
        "\n",
        "                        # If region already in dict, append new item_id to the value\n",
        "                        region_to_item[row.region_id].append(item_to_idx.get(row.item_id))  \n",
        "            \n",
        "            # Iterate through user region pairs, and find all items in those regions\n",
        "            for row in user_region_df.itertuples():\n",
        "                \n",
        "                # Building dict with cached_region_ids as key and all items for that region as value\n",
        "                if row.cached_region_ids not in region_str_to_items:\n",
        "                    \n",
        "                    # Unpack string into list of region_ids\n",
        "                    this_region_list = row.cached_region_ids.split(\",\")\n",
        "                    \n",
        "                    # Create empty list of all items in list of regions \n",
        "                    this_region_list_items = []\n",
        "                    \n",
        "                    # Iterate over regions and add all items to list\n",
        "                    for this_region in this_region_list:\n",
        "                        \n",
        "                        # Adds all items associated with this_region, adds nothing if key not found\n",
        "                        this_region_list_items.extend(region_to_item.get(int(this_region), []))\n",
        "                        \n",
        "                    # If all regions in list collectively have 0 items, replace with default list \n",
        "                    if this_region_list_items == []:\n",
        "                        this_region_list_items = default_item_list\n",
        "                    \n",
        "                    # Add the full list to the dictionary\n",
        "                    region_str_to_items[row.cached_region_ids] = this_region_list_items\n",
        "                \n",
        "                # Translate the user_id to the user_idx\n",
        "                this_user_idx = user_to_idx[row.user_id] # Should this be a .get? what to default?\n",
        "                \n",
        "                # Building dict with user_id as key and all items associated with their regions as value\n",
        "                user_to_region_item_idx[this_user_idx] = region_str_to_items.get(row.cached_region_ids, default_item_list)\n",
        "                \n",
        "                # Building a dict with user_id as key and number of items associated with their regions as value\n",
        "                user_to_num_items[this_user_idx] = len(region_str_to_items.get(row.cached_region_ids, default_num_items))\n",
        "\n",
        "            # Populate all users without region data with all items\n",
        "            for user_idx in training_set:\n",
        "                \n",
        "                if user_idx not in user_to_region_item_idx:\n",
        "                    user_to_region_item_idx[user_idx] = default_item_list\n",
        "                    user_to_num_items[user_idx] = default_num_items\n",
        "        \n",
        "        # Create deep copy of training set before removing test and val for deployment\n",
        "        deploy_set = copy.deepcopy(training_set)\n",
        "        \n",
        "        # Init lists of datasets\n",
        "        training_times = {}\n",
        "        \n",
        "        # Only init val lists if they'll be used\n",
        "        if self.args.val_set == 1:\n",
        "            val_set = {} \n",
        "            val_times = {}\n",
        "        \n",
        "        test_set = {}\n",
        "        test_times = {}\n",
        "        # Map from user to set of items for easy lookup\n",
        "        item_set_per_user = {}\n",
        "        \n",
        "        if self.args.val_set == 1:\n",
        "          print(\"Trying to structure dataset with validation set, this isn't fully supported. \")\n",
        "\n",
        "        for user in training_set:\n",
        "            \n",
        "            # If user has inadequate data for train/test split, use dummy values. (if val_set == 0, only 1 train, 1 test needed)\n",
        "            if len(list(training_set[user])) < (2 + self.args.val_set):\n",
        "                \n",
        "                # Reviewed < 3 items, insert dummy values\n",
        "                test_set[user] = (-1, -1)\n",
        "                test_times[user] = (-1, -1)\n",
        "                \n",
        "                if self.args.val_set == 1:\n",
        "                    val_set[user] = (-1, -1)\n",
        "                    val_times[user] = (-1, -1)\n",
        "                \n",
        "            # User has adequate data, populate train, val, and test data\n",
        "            else:\n",
        "                \n",
        "                # No validation set needed, only populate train/test\n",
        "                if self.args.val_set == 0:\n",
        "                    \n",
        "                    # Remove last item from train to serve as test\n",
        "                    test_item, test_time = training_set[user].pop() \n",
        "                    \n",
        "                    # This lookback is what requires the starting at 1 indexing\n",
        "                    last_item, last_time = training_set[user][-1] \n",
        "                    \n",
        "                    # Test item is the most recent item by user, last item is the one previous\n",
        "                    test_set[user] = (test_item, last_item) \n",
        "                    \n",
        "                    # Time functionality currently not supported\n",
        "                    test_times[user] = (test_time, last_time)\n",
        "                \n",
        "                #note: currently not well maintained, will need adjustments to work with some new functionality \n",
        "                else:\n",
        "                    test_item, test_time = training_set[user].pop() #remove last item from train to serve as test\n",
        "                    val_item, val_time = training_set[user].pop() #remove second to last item from train to serve as val\n",
        "                    last_item, last_time = training_set[user][-1] #this lookback is what requires the starting at 1 indexing \n",
        "                    test_set[user] = (test_item, val_item) #test item is the most recent item by user, val item is the one previous\n",
        "                    test_times[user] = (test_time, val_time)\n",
        "                    val_set[user] = (val_item, last_item) #val item is second to last item by usaer, 'last item' is third to last (last in training set) \n",
        "                    val_times[user] = (val_time, last_time)\n",
        "                    \n",
        "            # Separate timestamps and create item set\n",
        "            training_times[user] = copy.deepcopy(training_set[user])\n",
        "            training_set[user] = [x[0] for x in training_set[user]]\n",
        "            item_set_per_user[user] = set(training_set[user])\n",
        "            \n",
        "        # Iterate over users to get total count of training items\n",
        "        num_train_items = 0\n",
        "        for user in training_set:\n",
        "            num_train_items += len(list(training_set[user]))\n",
        "\n",
        "        # Set newly created datasets, dictionaries, and counts as dataset attributes\n",
        "        self.deploy_set = deploy_set\n",
        "        # self.deploy_times = deploy_times\n",
        "        \n",
        "        self.training_set = training_set\n",
        "        self.training_times = training_times\n",
        "        \n",
        "        if self.args.val_set == 1:\n",
        "            self.val_set = val_set\n",
        "            self.val_times = val_times\n",
        "        \n",
        "        self.test_set = test_set\n",
        "        self.test_times = test_times\n",
        "        self.item_set_per_user = item_set_per_user\n",
        "\n",
        "        self.item_to_idx = item_to_idx\n",
        "        self.user_to_idx = user_to_idx\n",
        "        self.idx_to_item = idx_to_item\n",
        "        self.idx_to_user = idx_to_user\n",
        "        \n",
        "        if type(self.args.user_region_df) == pd.DataFrame:\n",
        "            self.item_to_region = item_to_region\n",
        "            self.region_to_item = region_to_item\n",
        "            self.user_to_region_item_idx = user_to_region_item_idx\n",
        "            self.region_str_to_items = region_str_to_items\n",
        "            self.user_to_num_items = user_to_num_items\n",
        "            self.user_region_df = user_region_df\n",
        "\n",
        "        \n",
        "        self.num_train_items = num_train_items\n",
        "\n",
        "\n",
        "        # Full_features replaced 'content'\n",
        "        if self.args.features == 'full_features':\n",
        "          \n",
        "            # Place index on users df and set as attribute of dataset\n",
        "            print('Reading user demographics...')\n",
        "            user_df = self.args.user_df\n",
        "            user_df = user_df.set_index('user_id')\n",
        "            self.user_df = user_df\n",
        "\n",
        "            # Create dictionary to build sparse user feature matrix\n",
        "            self.orig_indices = []\n",
        "            for i in range(1, self.num_users):\n",
        "                self.orig_indices.append(self.idx_to_user[i])\n",
        "            self.user_feats = sp.csr_matrix(user_df.loc[self.orig_indices].values)\n",
        "          \n",
        "            # Repeat above for items instead of users\n",
        "            print('Reading item demographics...')\n",
        "            self.item_df = self.item_df.set_index('item_id')\n",
        "            self.orig_item_indices = []\n",
        "            for i in range(1, self.num_items):\n",
        "                self.orig_item_indices.append(self.idx_to_item[i])\n",
        "            self.item_feats = sp.csr_matrix(self.item_df.loc[self.orig_item_indices].values)\n",
        "        \n",
        "        else:\n",
        "            self.user_feats = None\n",
        "            self.item_feats = None\n",
        "            \n",
        "        # Create scipy.sparse matrices #NOTE: This is where indexing fix matters\n",
        "        self.user_one_hot = sp.identity(self.num_users - 0).tocsr()\n",
        "        self.item_one_hot = sp.identity(self.num_items - 0).tocsr()\n",
        "        \n",
        "        for user in deploy_set:\n",
        "\n",
        "            # Separate timestamps and create item set\n",
        "            deploy_set[user] = [x[0] for x in deploy_set[user]]\n",
        "        \n",
        "        # Sparse training matrices\n",
        "        train_rows = []\n",
        "        train_cols = []\n",
        "        train_vals = []\n",
        "        train_prev_vals = []\n",
        "        train_times = []\n",
        "        train_prev_times = []\n",
        "        \n",
        "        # Init list for generating negative samples #note - must happen after .pop() to avoid leakage\n",
        "        weighted_item_list = []\n",
        "        \n",
        "        # Restructure data into training rows with previous items as feature\n",
        "        for user in self.training_set:\n",
        "\n",
        "            # Start with 1st item instead of 0th item of training data to allow for prev item reference\n",
        "            for i in range(1, len(list(self.training_set[user]))):\n",
        "                \n",
        "                item = self.training_set[user][i]\n",
        "                item_prev = self.training_set[user][i-1]\n",
        "                item_time = self.training_times[user][i]\n",
        "                item_prev_time = self.training_times[user][i-1]\n",
        "                train_rows.append(user)\n",
        "                train_cols.append(item)\n",
        "                train_vals.append(1)\n",
        "                train_prev_vals.append(item_prev)\n",
        "                train_times.append(item_time[1])\n",
        "                train_prev_times.append(item_prev_time[1])\n",
        "                \n",
        "                # Add one observation to weighted item list\n",
        "                weighted_item_list.append(item) \n",
        "                \n",
        "        # Normalize values then set weights as attribute for negative sampling\n",
        "        self.weighted_item_list = weighted_item_list\n",
        "        \n",
        "        # Determine mean and std to normalize timestamps\n",
        "        self.train_mean = np.mean(train_times)\n",
        "        self.train_std  = np.std(train_times)\n",
        "        self.ONE_YEAR = (60 * 60 * 24 * 365) / self.train_mean\n",
        "        self.ONE_DAY = (60 * 60 * 24) / self.train_mean\n",
        "        train_times = (train_times - self.train_mean) / self.train_std\n",
        "\n",
        "        self.sp_train = sp.coo_matrix((train_vals, (train_rows, train_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        self.sp_train_prev = sp.coo_matrix((train_prev_vals, (train_rows, train_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        self.sp_train_times = sp.coo_matrix((train_times, (train_rows, train_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        self.sp_train_prev_times = sp.coo_matrix((train_prev_times, (train_rows, train_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        \n",
        "        # Repeat training processing for validation set\n",
        "        if self.args.val_set == 1:\n",
        "            \n",
        "            # Sparse validation matrices\n",
        "            val_rows = []\n",
        "            val_cols = []\n",
        "            val_vals = []\n",
        "            val_prev_vals = []\n",
        "            val_times = []\n",
        "            val_prev_times = []\n",
        "            for user in self.val_set:\n",
        "                item = self.val_set[user][0]\n",
        "                item_prev = self.val_set[user][1]\n",
        "                item_time = self.val_times[user][0]\n",
        "                item_prev_time = self.val_times[user][1]\n",
        "                if item == -1 or item_prev == -1:\n",
        "                    continue\n",
        "\n",
        "                val_rows.append(user)\n",
        "                val_cols.append(item)\n",
        "                val_vals.append(1)\n",
        "                val_prev_vals.append(item_prev)\n",
        "                val_times.append(item_time)\n",
        "                val_prev_times.append(item_prev_time)\n",
        "\n",
        "            #normalize val timestamps with train mean/std (avoid leakage)    \n",
        "            val_times = (val_times - self.train_mean) / self.train_std\n",
        "                \n",
        "            self.sp_val = sp.coo_matrix((val_vals, (val_rows, val_cols)),\n",
        "                    shape=(self.num_users, self.num_items))\n",
        "            self.sp_val_prev = sp.coo_matrix((val_prev_vals, (val_rows, val_cols)),\n",
        "                    shape=(self.num_users, self.num_items))\n",
        "            self.sp_val_times = sp.coo_matrix((val_times, (val_rows, val_cols)),\n",
        "                    shape=(self.num_users, self.num_items))\n",
        "            self.sp_val_prev_times = sp.coo_matrix((val_prev_times, (val_rows, val_cols)),\n",
        "                    shape=(self.num_users, self.num_items))\n",
        "\n",
        "        # Repeat processing for test set\n",
        "        test_rows = []\n",
        "        test_cols = []\n",
        "        test_vals = []\n",
        "        test_prev_vals = []\n",
        "        test_times = []\n",
        "        test_prev_times = []\n",
        "        for user in self.test_set:\n",
        "            item = self.test_set[user][0] #for test and val set, this_item is 0\n",
        "            item_prev = self.test_set[user][1] #prev_item is 1\n",
        "            item_time = self.test_times[user][0]\n",
        "            item_prev_time = self.test_times[user][1]\n",
        "            if item == -1 or item_prev == -1:\n",
        "                continue\n",
        "\n",
        "            test_rows.append(user)\n",
        "            test_cols.append(item)\n",
        "            test_vals.append(1)\n",
        "            test_prev_vals.append(item_prev)\n",
        "            test_times.append(item_time)\n",
        "            test_prev_times.append(item_prev_time)\n",
        "\n",
        "        # Normalize test timestamps with train mean/std (avoid leakage)    \n",
        "        test_times = (test_times - self.train_mean) / self.train_std\n",
        "            \n",
        "        self.sp_test = sp.coo_matrix((test_vals, (test_rows, test_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        self.sp_test_prev = sp.coo_matrix((test_prev_vals, (test_rows, test_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        self.sp_test_times = sp.coo_matrix((test_times, (test_rows, test_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        self.sp_test_prev_times = sp.coo_matrix((test_prev_times, (test_rows, test_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "\n",
        "        # Sparse training matrices for deploy set #old deploy method\n",
        "        deploy_rows = []\n",
        "        deploy_cols = []\n",
        "        deploy_vals = []\n",
        "        deploy_prev_vals = []\n",
        "        \n",
        "        #deploy_times = []\n",
        "        #deploy_prev_times = []\n",
        "        \n",
        "        # Check if region data should be used\n",
        "        if type(self.args.user_region_df) == pd.DataFrame:\n",
        "            \n",
        "            # Create empty dict to find index user starts at for efficient user lookups\n",
        "            deploy_user_start_idx = {}\n",
        "            \n",
        "            # Start dict with user 0's starting point, index 0\n",
        "            deploy_user_start_idx[0] = 0\n",
        "            \n",
        "            # Iterate over the copy of the training set, for previous items and user feats\n",
        "            for user in self.deploy_set:\n",
        "                \n",
        "                # Find index of last item, to populate 'prev_item'\n",
        "                last_item_idx = len(list(self.deploy_set[user])) - 1\n",
        "                \n",
        "                item_prev = self.deploy_set[user][last_item_idx - 1]\n",
        "                \n",
        "                this_user_items = self.user_to_region_item_idx.get(user)\n",
        "                \n",
        "                # The next user will start after this one, so add this user's num rows to this user's start idx\n",
        "                deploy_user_start_idx[user + 1] = len(this_user_items) + deploy_user_start_idx[user]\n",
        "                \n",
        "                for item in this_user_items:\n",
        "                    \n",
        "                    # For each item add a row for this user, their prev item, a positive, and each item\n",
        "                    deploy_rows.append(user)\n",
        "                    deploy_cols.append(item)\n",
        "                    deploy_vals.append(1)\n",
        "                    deploy_prev_vals.append(item_prev)\n",
        "                \n",
        "        # If region data isn't being used, build exhaustive list of all user, item pairs\n",
        "        else:\n",
        "            \n",
        "            # Create empty dict to find index user starts at for efficient user lookups\n",
        "            deploy_user_start_idx = {}\n",
        "            \n",
        "            # Start dict with user 0's starting point, index 0\n",
        "            deploy_user_start_idx[0] = 0\n",
        "\n",
        "            # Iterate over the copy of the training set, for previous items and user feats\n",
        "            for user in self.deploy_set:\n",
        "\n",
        "                # Find index of last item, to populate 'prev_item'\n",
        "                last_item_idx = len(list(self.deploy_set[user])) - 1\n",
        "\n",
        "                item_prev = self.deploy_set[user][last_item_idx - 1]\n",
        "\n",
        "                # The next user will start after this one, so add this user's num rows to this user's start idx\n",
        "                deploy_user_start_idx[user + 1] = self.num_items + deploy_user_start_idx[user]\n",
        "\n",
        "                for item in range(self.num_items):\n",
        "\n",
        "                    # For each item add a row for this user, their prev item, a positive, and each item\n",
        "                    deploy_rows.append(user)\n",
        "                    deploy_cols.append(item)\n",
        "                    deploy_vals.append(1)\n",
        "                    deploy_prev_vals.append(item_prev)\n",
        "\n",
        "        self.deploy_user_start_idx = deploy_user_start_idx\n",
        "        self.deploy_rows = deploy_rows\n",
        "        self.deploy_cols = deploy_cols\n",
        "        self.deploy_vals = deploy_vals\n",
        "        self.deploy_prev_vals = deploy_prev_vals\n",
        "        \n",
        "        # Assign the totla number of rows in deploy set as model attribute, to determine batch size\n",
        "        self.deploy_num_rows = len(deploy_rows)\n",
        "        \n",
        "    # Function to generate model input training data, actual values\n",
        "    def generate_pos_train_batch_sp(self, ith_seed = 1, items_per_user = 3):\n",
        "        \n",
        "        np.random.seed(ith_seed)\n",
        "\n",
        "        # Subtract 1 to account for missing 0 index\n",
        "        user_indices = np.repeat(self.sp_train.row, items_per_user) - 1\n",
        "        prev_indices = np.repeat(self.sp_train_prev.data, items_per_user) - 1\n",
        "        pos_indices = np.repeat(self.sp_train.col, items_per_user) - 1\n",
        "        \n",
        "        # Convert from indices to one hot matrices\n",
        "        pos_users = self.user_one_hot[user_indices]\n",
        "        prev_items = self.item_one_hot[prev_indices]\n",
        "        pos_items = self.item_one_hot[pos_indices]\n",
        "\n",
        "        # Horizontally stack sparse matrices to get single positive\n",
        "        pos_feats = sp.hstack([pos_users, prev_items, pos_items])\n",
        "            \n",
        "        # Full_features replaced 'content', adds both user and item features\n",
        "        if self.args.features == 'full_features':\n",
        "            # Join with content data            \n",
        "            user_content = self.user_feats[user_indices]\n",
        "            pos_item_content = self.item_feats[pos_indices]\n",
        "            pos_feats = sp.hstack([pos_feats, user_content, pos_item_content])\n",
        "\n",
        "        return(pos_users, pos_feats)\n",
        "      \n",
        "    # Generate random observations of the same size as actual training input\n",
        "    def generate_neg_train_batch_sp(self, ith_seed = 1, items_per_user = 3):\n",
        "        \n",
        "        np.random.seed(ith_seed)\n",
        "        \n",
        "        # Subtract 1 to account for missing 0 index\n",
        "        user_indices = np.repeat(self.sp_train.row, items_per_user) - 1\n",
        "        prev_indices = np.repeat(self.sp_train_prev.data, items_per_user) - 1\n",
        "        \n",
        "        # Check if region data should be used\n",
        "        if type(self.args.user_region_df) == pd.DataFrame:\n",
        "            neg_indices = []\n",
        "            \n",
        "            # Iterate over each user index to build training data for only this region\n",
        "            for user_idx in user_indices:\n",
        "                \n",
        "                # Unpacked list of lists generated above into a single list of elligible items for user\n",
        "                #     If no regions associated with user, defdault to all items\n",
        "                all_user_items = self.user_to_region_item_idx.get(user_idx + 1, [])\n",
        "                \n",
        "                # Randomly sample a single element from the list\n",
        "                rand_idx = random.randint(0, len(all_user_items) - 1)\n",
        "                neg_indices.append(all_user_items[rand_idx] - 1)\n",
        "                \n",
        "        \n",
        "        elif self.args.weighted_sampling == 1:\n",
        "            neg_indices = np.random.choice(range(len(self.weighted_item_list)), \n",
        "                          size = len(self.sp_train.row) * items_per_user) \n",
        "        \n",
        "            neg_indices = [(self.weighted_item_list[i] - 1) for i in neg_indices] \n",
        "            \n",
        "        else:\n",
        "            neg_indices = np.random.randint(1, self.sp_train.shape[1],\n",
        "                          size=len(self.sp_train.row)*items_per_user, dtype=np.int32) - 1\n",
        "        \n",
        "        # Convert from indices to one hot matrices\n",
        "        neg_users = self.user_one_hot[user_indices]\n",
        "        prev_items = self.item_one_hot[prev_indices]\n",
        "        neg_items = self.item_one_hot[neg_indices]\n",
        "\n",
        "        # Horizontally stack sparse matrices to get negative feature matrices\n",
        "        neg_feats = sp.hstack([neg_users, prev_items, neg_items])\n",
        "            \n",
        "        # Full_features replaced 'content', adds both user and item features\n",
        "        if self.args.features == 'full_features':\n",
        "            # Join with content data            \n",
        "            user_content = self.user_feats[user_indices]\n",
        "            neg_item_content = self.item_feats[neg_indices]\n",
        "            neg_feats = sp.hstack([neg_feats, user_content, neg_item_content])\n",
        "\n",
        "        return(neg_users, neg_feats)\n",
        "    \n",
        "    # Dataset containing only correct inputs, model should how to score these well\n",
        "    def generate_pos_val_batch_sp(self, ith_seed = 1): \n",
        "        \n",
        "        np.random.seed(ith_seed)\n",
        "        \n",
        "        user_indices = self.sp_val.row - 1\n",
        "        prev_indices = self.sp_val_prev.data - 1\n",
        "        pos_indices = self.sp_val.col - 1\n",
        "\n",
        "        # Convert from indices to one-hot matrices\n",
        "        pos_users = self.user_one_hot[user_indices]\n",
        "        prev_items = self.item_one_hot[prev_indices]\n",
        "        pos_items = self.item_one_hot[pos_indices]\n",
        "\n",
        "        # Horizontally stack sparse matrices to get single positive feats\n",
        "        pos_feats = sp.hstack([pos_users, prev_items, pos_items])\n",
        "\n",
        "        # Full_features replaced 'content', adds both user and item features\n",
        "        if self.args.features == 'full_features':\n",
        "            # Join with content data\n",
        "            user_content = self.user_feats[user_indices]\n",
        "            pos_item_content = self.item_feats[pos_indices]\n",
        "            pos_feats = sp.hstack([pos_feats, user_content, pos_item_content])\n",
        "\n",
        "        return(pos_users, pos_feats)\n",
        "      \n",
        "    # Dataset containing random samples, model should learn these are less likely than the above\n",
        "    def generate_neg_val_batch_sp(self, ith_seed = 1, items_per_user = 10): \n",
        "        \n",
        "        np.random.seed(ith_seed)\n",
        "        \n",
        "        user_indices = np.repeat(self.sp_val.row, items_per_user) - 1\n",
        "        prev_indices = np.repeat(self.sp_val_prev.data, items_per_user) - 1\n",
        "        \n",
        "        # Check if region data should be used\n",
        "        if type(self.args.user_region_df) == pd.DataFrame:\n",
        "            neg_indices = []\n",
        "            \n",
        "            # Iterate over each user index to build training data for only this region\n",
        "            for user_idx in user_indices:\n",
        "                \n",
        "                # Unpacked list of lists generated above into a single list of elligible items for user\n",
        "                #     If no regions associated with user, defdault to all items\n",
        "                all_user_items = self.user_to_region_item_idx.get(user_idx + 1, [])\n",
        "                \n",
        "                # Randomly sample a single element from the list\n",
        "                rand_idx = random.randint(0, len(all_user_items) - 1)\n",
        "                neg_indices.append(all_user_items[rand_idx] - 1)\n",
        "                \n",
        "        \n",
        "        elif self.args.weighted_sampling == 1:\n",
        "            neg_indices = np.random.choice(range(len(self.weighted_item_list)), \n",
        "                size = len(self.sp_val.row)*items_per_user) #- 1\n",
        "        \n",
        "            neg_indices = [(self.weighted_item_list[i] - 1) for i in neg_indices] \n",
        "        \n",
        "        else:\n",
        "            neg_indices = np.random.randint(1, self.sp_val.shape[1],\n",
        "                          size=len(self.sp_val.row)*items_per_user, dtype=np.int32) - 1\n",
        "        \n",
        "        self.neg_ind = neg_indices\n",
        "        \n",
        "        # Convert from indices to one-hot matrices\n",
        "        neg_users = self.user_one_hot[user_indices]\n",
        "        prev_items = self.item_one_hot[prev_indices]\n",
        "        neg_items = self.item_one_hot[neg_indices]\n",
        "        \n",
        "        # Horizontally stack sparse matrices to get negative feature matrices\n",
        "        neg_feats = sp.hstack([neg_users, prev_items, neg_items])\n",
        "        \n",
        "        # Full_features replaced 'content', adds both user and item features\n",
        "        if self.args.features == 'full_features':\n",
        "            # Join with content data\n",
        "            user_content = self.user_feats[user_indices]\n",
        "            neg_item_content = self.item_feats[neg_indices]\n",
        "            neg_feats = sp.hstack([neg_feats, user_content, neg_item_content])\n",
        "            \n",
        "        return (neg_users, neg_feats)\n",
        "\n",
        "    # Dataset containing only correct inputs, model should how to score these well\n",
        "    def generate_pos_test_batch_sp(self, ith_seed = 1): \n",
        "        \n",
        "        np.random.seed(ith_seed)\n",
        "        \n",
        "        user_indices = self.sp_test.row - 1\n",
        "        prev_indices = self.sp_test_prev.data - 1\n",
        "        pos_indices = self.sp_test.col - 1\n",
        "\n",
        "        # Convert from indices to one-hot matrices\n",
        "        pos_users = self.user_one_hot[user_indices]\n",
        "        prev_items = self.item_one_hot[prev_indices]\n",
        "        pos_items = self.item_one_hot[pos_indices]\n",
        "\n",
        "        # Horizontally stack sparse matrices to get single positive feats\n",
        "        pos_feats = sp.hstack([pos_users, prev_items, pos_items])\n",
        "\n",
        "        # Full_features replaced 'content', adds both user and item features\n",
        "        if self.args.features == 'full_features':\n",
        "            # Join with content data\n",
        "            user_content = self.user_feats[user_indices]\n",
        "            pos_item_content = self.item_feats[pos_indices]\n",
        "            pos_feats = sp.hstack([pos_feats, user_content, pos_item_content])\n",
        "\n",
        "        return(pos_users, pos_feats)\n",
        "      \n",
        "    # Dataset containing random samples, model should learn these are less likely than the above\n",
        "    def generate_neg_test_batch_sp(self, ith_seed = 1, items_per_user = 10): \n",
        "        \n",
        "        np.random.seed(ith_seed)\n",
        "        \n",
        "        user_indices = np.repeat(self.sp_test.row, items_per_user) - 1\n",
        "        prev_indices = np.repeat(self.sp_test_prev.data, items_per_user) - 1\n",
        "        \n",
        "        # Check if region data should be used\n",
        "        if type(self.args.user_region_df) == pd.DataFrame:\n",
        "            neg_indices = []\n",
        "            \n",
        "            # Iterate over each user index to build training data for only this region\n",
        "            for user_idx in user_indices:\n",
        "                \n",
        "                # Unpacked list of lists generated above into a single list of elligible items for user\n",
        "                #     If no regions associated with user, defdault to all items\n",
        "                all_user_items = self.user_to_region_item_idx.get(user_idx + 1, [])\n",
        "                \n",
        "                # Randomly sample a single element from the list\n",
        "                rand_idx = random.randint(0, len(all_user_items) - 1)\n",
        "                neg_indices.append(all_user_items[rand_idx] - 1)\n",
        "                \n",
        "        \n",
        "        elif self.args.weighted_sampling == 1:\n",
        "            neg_indices = np.random.choice(range(len(self.weighted_item_list)), \n",
        "                size = len(self.sp_test.row)*items_per_user) #- 1\n",
        "        \n",
        "            neg_indices = [(self.weighted_item_list[i] - 1) for i in neg_indices] \n",
        "        \n",
        "        else:\n",
        "            neg_indices = np.random.randint(1, self.sp_test.shape[1],\n",
        "                          size=len(self.sp_test.row)*items_per_user, dtype=np.int32) - 1\n",
        "        \n",
        "        self.neg_ind = neg_indices\n",
        "        \n",
        "        # Convert from indices to one-hot matrices\n",
        "        neg_users = self.user_one_hot[user_indices]\n",
        "        prev_items = self.item_one_hot[prev_indices]\n",
        "        neg_items = self.item_one_hot[neg_indices]\n",
        "        \n",
        "        # Horizontally stack sparse matrices to get negative feature matrices\n",
        "        neg_feats = sp.hstack([neg_users, prev_items, neg_items])\n",
        "        \n",
        "        # Full_features replaced 'content', adds both user and item features\n",
        "        if self.args.features == 'full_features':\n",
        "            # Join with content data\n",
        "            user_content = self.user_feats[user_indices]\n",
        "            neg_item_content = self.item_feats[neg_indices]\n",
        "            neg_feats = sp.hstack([neg_feats, user_content, neg_item_content])\n",
        "            \n",
        "        return (neg_users, neg_feats)\n",
        "        \n",
        "    # All user, item pairs, to be evaluated in chunks defined by idx_sample\n",
        "    def generate_deploy_batch_sp(self, idx_sample, one_pass = 1): \n",
        "   \n",
        "        this_deploy_rows = [self.deploy_rows[i] for i in idx_sample]\n",
        "        this_deploy_cols = [self.deploy_cols[i] for i in idx_sample]\n",
        "        this_deploy_vals = [self.deploy_vals[i] for i in idx_sample]\n",
        "        this_deploy_prev_vals = [self.deploy_prev_vals[i] for i in idx_sample]\n",
        "        \n",
        "        this_sp_deploy = sp.coo_matrix((this_deploy_vals, (this_deploy_rows, this_deploy_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        this_sp_deploy_prev = sp.coo_matrix((this_deploy_prev_vals, (this_deploy_rows, this_deploy_cols)),\n",
        "                shape=(self.num_users, self.num_items))\n",
        "        \n",
        "        # Subtract 1 to account for missing 0 index\n",
        "        user_indices = this_sp_deploy.row - 1\n",
        "        prev_indices = this_sp_deploy_prev.data - 1\n",
        "        deploy_indices = this_sp_deploy.col - 1\n",
        "\n",
        "        # Convert from indices to one hot matrices\n",
        "        pos_users = self.user_one_hot[user_indices]\n",
        "        prev_items = self.item_one_hot[prev_indices]\n",
        "        pos_items = self.item_one_hot[deploy_indices]\n",
        "\n",
        "        # Horizontally stack sparse matrices to get single positive feature matrices\n",
        "        pos_feats = sp.hstack([pos_users, prev_items, pos_items])\n",
        "\n",
        "        # Full_features replaced 'content', adds both user and item features\n",
        "        if self.args.features == 'full_features':\n",
        "            # Join with content data            \n",
        "            user_content = self.user_feats[user_indices]\n",
        "            pos_item_content = self.item_feats[deploy_indices]\n",
        "            pos_feats = sp.hstack([pos_feats, user_content, pos_item_content])\n",
        "\n",
        "        return(pos_users, pos_feats)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeR2jGYYN9Ke"
      },
      "source": [
        "Recommender Object\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soHIknLUm71G"
      },
      "source": [
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# DEBUG - for time stamps\n",
        "import time\n",
        "\n",
        "# A layer object, where one layer represents the entire PRME algorithm\n",
        "class PRME(layers.Layer):\n",
        "\n",
        "    # When initialized, create the two lower-dimenstional matrices to approximate data\n",
        "    def __init__(self, input_dim, factor_dim, seed):\n",
        "\n",
        "        # Set random seed for reproducability\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "        # Both matrices init as random, then the model learns how they can better represent data\n",
        "        super(PRME, self).__init__()\n",
        "        lin_init = tf.random_normal_initializer()\n",
        "\n",
        "        # Var_linear can be thought of as the linear bias to any comparisons\n",
        "        self.var_linear = tf.Variable(initial_value=lin_init(shape=(input_dim, 1),\n",
        "                                                  dtype='float32'), trainable=True)\n",
        "\n",
        "        # Var_factors can optionally have multiple dimenstions and represent a more complex space\n",
        "        factor_init = tf.random_normal_initializer()\n",
        "        self.var_factors = tf.Variable(initial_value=factor_init(shape=(input_dim, factor_dim),\n",
        "                                                  dtype='float32'), trainable=True)\n",
        "\n",
        "    # When the model is called, expects features as input and returns scores for each item, user pair\n",
        "    def call(self, sparse_feats):\n",
        "        linear_bias = tf.sparse.sparse_dense_matmul(sparse_feats, self.var_linear)\n",
        "        var_emb_product = tf.reduce_sum(tf.square(self.var_factors), axis=1, keepdims = True)\n",
        "\n",
        "        feats_sum = tf.sparse.reduce_sum(sparse_feats, axis=1, keepdims = True)\n",
        "        emb_mul = tf.sparse.sparse_dense_matmul(sparse_feats, self.var_factors)\n",
        "        # Term 1\n",
        "        prod_term = tf.sparse.sparse_dense_matmul(sparse_feats, var_emb_product)\n",
        "        term_1 = prod_term * feats_sum\n",
        "        # Term 2\n",
        "        term_2 = 2 * tf.reduce_sum(tf.square(emb_mul), axis=1, keepdims = True)\n",
        "        # Term 3\n",
        "        term_3 = term_1\n",
        "        # Predictions\n",
        "        preds = linear_bias + 0.5 * (term_1 - term_2 + term_3)\n",
        "        return(preds)\n",
        "\n",
        "# A layer object, where one layer represents the entire PRME algorithm\n",
        "class TransFM(layers.Layer):\n",
        "\n",
        "    # When initialized, create the two lower-dimenstional matrices to approximate data\n",
        "    def __init__(self, input_dim, factor_dim, seed):\n",
        "\n",
        "        # Set random seed for reproducability\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "        # Both matrices init as random, then the model learns how they can better represent data\n",
        "        super(TransFM, self).__init__()\n",
        "        lin_init = tf.random_normal_initializer()\n",
        "\n",
        "        # Var_linear can be thought of as the linear bias to any comparisons\n",
        "        self.var_linear = tf.Variable(initial_value=lin_init(shape=(input_dim, 1),\n",
        "                                                  dtype='float32'), trainable=True)\n",
        "\n",
        "        # Var_emb_factors and var_trans_factors can optionally have multiple dimenstions\n",
        "        factor_init = tf.random_normal_initializer()\n",
        "        self.var_emb_factors = tf.Variable(initial_value=factor_init(shape=(input_dim, factor_dim),\n",
        "                                                  dtype='float32'), trainable=True)\n",
        "\n",
        "        self.var_trans_factors = tf.Variable(initial_value=factor_init(shape=(input_dim, factor_dim),\n",
        "                                                  dtype='float32'), trainable=True)\n",
        "\n",
        "    # When the model is called, expects features as input and returns scores for each item, user pair\n",
        "    def call(self, sparse_feats):\n",
        "        linear_bias = tf.sparse.sparse_dense_matmul(sparse_feats, self.var_linear)\n",
        "        var_emb_product = tf.reduce_sum(tf.square(self.var_emb_factors), axis=1, keepdims = True)\n",
        "\n",
        "        var_trans_product = tf.reduce_sum(tf.square(self.var_trans_factors), axis=1, keepdims = True)\n",
        "        var_emb_trans_product = tf.reduce_sum(tf.math.multiply(self.var_emb_factors, self.var_trans_factors),\n",
        "                axis=1, keepdims=True)\n",
        "\n",
        "        feats_sum = tf.sparse.reduce_sum(sparse_feats, axis=1, keepdims = True)\n",
        "        emb_mul = tf.sparse.sparse_dense_matmul(sparse_feats, self.var_emb_factors)\n",
        "        trans_mul = tf.sparse.sparse_dense_matmul(sparse_feats, self.var_trans_factors)\n",
        "\n",
        "        # Term 1\n",
        "        prod_term = tf.sparse.sparse_dense_matmul(sparse_feats, var_emb_product)\n",
        "        term_1 = prod_term * feats_sum\n",
        "\n",
        "        # Term 2\n",
        "        prod_term = tf.sparse.sparse_dense_matmul(sparse_feats, var_trans_product)\n",
        "        term_2 = prod_term * feats_sum\n",
        "\n",
        "        # Term 3\n",
        "        term_3 = term_1\n",
        "\n",
        "        # Term 4\n",
        "        prod_term = tf.sparse.sparse_dense_matmul(sparse_feats, var_emb_trans_product)\n",
        "        term_4 = 2 * prod_term * feats_sum\n",
        "\n",
        "        # Term 5\n",
        "        term_5 = 2 * tf.reduce_sum(tf.square(emb_mul), axis=1, keepdims=True)\n",
        "\n",
        "        # Term 6\n",
        "        term_6 = 2 * tf.reduce_sum(trans_mul * emb_mul, axis=1, keepdims=True)\n",
        "\n",
        "        # Diag term\n",
        "        diag_term = tf.reduce_sum(tf.square(trans_mul), axis=1, keepdims=True)\n",
        "\n",
        "        # Predictions\n",
        "        preds = linear_bias + 0.5 * (term_1 + term_2 + term_3\n",
        "                + term_4 - term_5 - term_6) - 0.5 * diag_term\n",
        "        return(preds)\n",
        "\n",
        "# The class representing the recommender, contains methods for training and recommending\n",
        "class Recommender:\n",
        "    def __init__(self, dataset, args, ckpt_path=\"./tf_ckpts\"):\n",
        "        self.dataset = dataset\n",
        "        self.args = args\n",
        "        self.ckpt_path = ckpt_path\n",
        "\n",
        "        # Use a training batch to figure out feature dimensionality and init inputs\n",
        "        pos_users, pos_feats = self.dataset.generate_pos_train_batch_sp(ith_seed = 1)\n",
        "        neg_users, neg_feats = self.dataset.generate_neg_train_batch_sp(ith_seed = 1)\n",
        "\n",
        "        # Format training data to create predictions, including sparse features\n",
        "        pos_users, sparse_pos_feats = self.feed_dict(pos_users, pos_feats)\n",
        "        neg_users, sparse_neg_feats = self.feed_dict(neg_users, neg_feats)\n",
        "\n",
        "        self.feature_dim = pos_feats.shape[1]\n",
        "        self.args.logger('Feature dimension = ' + str(self.feature_dim) + \"x\" + str(self.args.num_dims), 1)\n",
        "\n",
        "        # Init an empty keras model with Adam optimizer\n",
        "        self.model = tf.keras.Sequential()\n",
        "        self.opt = tf.keras.optimizers.Adam(learning_rate=self.args.starting_lr)\n",
        "\n",
        "        if self.args.model == \"PRME\":\n",
        "            # The only 'layer' is PRME, this declaration will initialize new random matricies\n",
        "            self.model.add(PRME(self.feature_dim, self.args.num_dims, self.args.random_seed))\n",
        "\n",
        "            # Increases as model quality increases, multiplied by -1 for minimization\n",
        "            prereg_loss = lambda: tf.reduce_sum(tf.math.log(1e-6 + tf.math.sigmoid(\n",
        "                    ((self.model(sparse_pos_feats) - self.model(sparse_neg_feats)) * args.secondary_reg_scale)))) * -1\n",
        "\n",
        "            # L2 regularization, using scaling passed in from args\n",
        "            l2_reg = lambda: tf.add_n([\n",
        "                tf.reduce_sum(tf.math.square(self.model.layers[0].var_linear)) * self.args.linear_reg,\n",
        "                tf.reduce_sum(tf.math.square(self.model.layers[0].var_factors)) * self.args.emb_reg\n",
        "            ])\n",
        "\n",
        "            # Total loss expressed as sum of model loss and l2 regularization\n",
        "            self.loss_fn = lambda: tf.add_n([prereg_loss(), l2_reg()])\n",
        "\n",
        "            # Save model and low-dimenstional features as attributes for later reference\n",
        "            self.var_linear = self.model.layers[0].var_linear\n",
        "            self.var_factors = self.model.layers[0].var_factors\n",
        "\n",
        "            # Declare variables to be included in checkpoint and save checkpoint and checkpoint manager as model attributes\n",
        "            self.ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer = self.opt, var_linear = self.var_linear,\n",
        "                                            var_factors = self.var_factors)\n",
        "            self.ckpt_manager = tf.train.CheckpointManager(self.ckpt, self.ckpt_path, max_to_keep=3)\n",
        "\n",
        "        if self.args.model == \"TransFM\":\n",
        "            # The only 'layer' is PRME, this declaration will initialize new random matricies\n",
        "            self.model.add(TransFM(self.feature_dim, self.args.num_dims, self.args.random_seed))\n",
        "\n",
        "            # Increases as model quality increases, multiplied by -1 for minimization\n",
        "            prereg_loss = lambda: tf.reduce_sum(tf.math.log(1e-6 + tf.math.sigmoid(\n",
        "                    ((self.model(sparse_pos_feats) - self.model(sparse_neg_feats)) * args.secondary_reg_scale)))) * -1\n",
        "\n",
        "            # L2 regularization, using scaling passed in from args\n",
        "            l2_reg = lambda: tf.add_n([\n",
        "                tf.reduce_sum(tf.math.square(self.model.layers[0].var_linear)) * self.args.linear_reg,\n",
        "                tf.reduce_sum(tf.math.square(self.model.layers[0].var_emb_factors)) * self.args.emb_reg,\n",
        "                tf.reduce_sum(tf.math.square(self.model.layers[0].var_trans_factors)) * self.args.trans_reg\n",
        "            ])\n",
        "\n",
        "            # Total loss expressed as sum of model loss and l2 regularization\n",
        "            self.loss_fn = lambda: tf.add_n([prereg_loss(), l2_reg()])\n",
        "\n",
        "            # Save model and low-dimenstional features as attributes for later reference\n",
        "            self.var_linear = self.model.layers[0].var_linear\n",
        "            self.var_emb_factors = self.model.layers[0].var_emb_factors\n",
        "            self.var_trans_factors = self.model.layers[0].var_trans_factors\n",
        "\n",
        "            # Declare variables to be included in checkpoint and save checkpoint and checkpoint manager as model attributes\n",
        "            self.ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer = self.opt, var_linear = self.var_linear,\n",
        "                                            var_emb_factors = self.var_emb_factors, var_trans_factors = self.var_trans_factors)\n",
        "            self.ckpt_manager = tf.train.CheckpointManager(self.ckpt, self.ckpt_path, max_to_keep=3)\n",
        "\n",
        "        # Declare the minimization should occur by modifying all trainable weights\n",
        "        self.var_list_fn = lambda: self.model.trainable_weights\n",
        "\n",
        "    # Structure user and object features into sparse inputs to model\n",
        "    def feed_dict(self, user_obj, feat_obj):\n",
        "        pl_users = user_obj.nonzero()[1]\n",
        "        pl_indices = np.hstack((feat_obj.nonzero()[0][:, None], feat_obj.nonzero()[1][:, None]))\n",
        "        pl_values = feat_obj.data.astype('float32')\n",
        "        pl_shape = feat_obj.shape\n",
        "        sparse_feats = tf.SparseTensor(pl_indices, pl_values, pl_shape)\n",
        "        return pl_users, sparse_feats\n",
        "\n",
        "    def format_output(self, user_list, item_list, score_list, json_out = False):\n",
        "\n",
        "        out_df = pd.DataFrame()\n",
        "        out_df['user_id'] = user_list\n",
        "        out_df['item_id'] = item_list\n",
        "        out_df['score'] = score_list\n",
        "\n",
        "        if json_out:\n",
        "            final_json = \"{\"\n",
        "            unique_ids = out_df['user_id'].unique()\n",
        "            for pos, user_id in enumerate(unique_ids):\n",
        "\n",
        "                final_json += \"\\\"{}\\\": {{\\\"\".format(user_id)\n",
        "                user_sample = out_df[out_df['user_id'] == user_id]\n",
        "                user_sample = user_sample.drop(columns = ['user_id'])\n",
        "                user_sample = user_sample.set_index('item_id')\n",
        "                user_json = user_sample.to_json(orient = 'columns')\n",
        "                # if items and scores are empty strings, replace with empty dictionary\n",
        "                if user_json[11] == \"\\\"\":\n",
        "                    final_json = final_json[:-1]\n",
        "                    final_json = final_json + \"}\"\n",
        "                else:\n",
        "                    final_json += user_json[11:-1]\n",
        "\n",
        "                if pos != len(unique_ids) - 1:\n",
        "                    final_json += \", \"\n",
        "                else:\n",
        "                    final_json += \"}\"\n",
        "            final_json = json.loads(final_json)\n",
        "\n",
        "            return final_json\n",
        "        else:\n",
        "            return out_df\n",
        "    # Restore the best model and generate predictions for all users and items\n",
        "    def user_preds(self, user_idx, item_whitelist=None):\n",
        "\n",
        "        # Find starting index in deploy_set\n",
        "        this_idx = range(self.dataset.deploy_user_start_idx[user_idx], self.dataset.deploy_user_start_idx[user_idx + 1])\n",
        "\n",
        "        # Generate user list and features for this sample\n",
        "        deploy_users, deploy_feats = self.dataset.generate_deploy_batch_sp(idx_sample = this_idx, one_pass = 0)\n",
        "        sparse_deploy_users, sparse_deploy_feats = self.feed_dict(deploy_users, deploy_feats)\n",
        "\n",
        "        # Run recommender and find scores for user, item pairs\n",
        "        deploy_preds = self.model(sparse_deploy_feats)\n",
        "\n",
        "        # Convert user index to user_id\n",
        "        user_id = self.dataset.idx_to_user[user_idx]\n",
        "\n",
        "        # List of user ids same length of input index\n",
        "        #user_list = [user_id for i in this_idx]\n",
        "\n",
        "        # Use the block of user, item pairs to create a list of item ids for that user\n",
        "        item_ids = [self.dataset.deploy_cols[i] for i in this_idx]\n",
        "\n",
        "        # List of all item_ids used as input\n",
        "        item_list = np.array([self.dataset.idx_to_item[i] for i in item_ids])\n",
        "\n",
        "        # List of all scores output by the model, formatted as floats\n",
        "        score_list = deploy_preds.numpy()\n",
        "\n",
        "        # Make sure top_k isn't larger than all items in region\n",
        "        adjusted_k = min(len(this_idx), self.args.return_k_preds)\n",
        "\n",
        "        # Find list of indices representing top k scores, sorted for largest values\n",
        "        top_k_indices = score_list.argsort(axis=0)[::-1][:adjusted_k]\n",
        "\n",
        "        # Find scores / items corresponding to highest indices\n",
        "        top_k_scores = [score_list[i][0][0] for i in top_k_indices]\n",
        "        top_k_items = [item_list[i][0] for i in top_k_indices]\n",
        "\n",
        "        # List of user ids same length of output\n",
        "        user_list = [user_id for i in top_k_indices]\n",
        "\n",
        "        return user_list, top_k_items, top_k_scores\n",
        "\n",
        "    def get_all_user_preds(self, final_user_list, final_item_list, final_score_list, this_chunk=0, num_chunks=1, user_id=None, json_out=False):\n",
        "        # iterate over all users (or partition of users) and produce predictions\n",
        "\n",
        "        # One more chunk then evenly divisible to ensure all users are represented\n",
        "        users_per_chunk = (self.dataset.num_users // num_chunks) + 1\n",
        "\n",
        "        user_start = users_per_chunk * this_chunk\n",
        "\n",
        "        user_end = users_per_chunk * (this_chunk + 1)\n",
        "\n",
        "        # Make sure end is at most total number of users\n",
        "        user_end = min(user_end, self.dataset.num_users)\n",
        "\n",
        "        # Create a prediction chunk for each iteration and merge into output\n",
        "        for this_user_idx in range(user_start, user_end):\n",
        "\n",
        "            # Pass in this user's index and return three lists of same length, to be merged into df\n",
        "            user_list, item_list, score_list = self.user_preds(user_idx = this_user_idx)\n",
        "\n",
        "            final_user_list.extend(user_list)\n",
        "            final_item_list.extend(item_list)\n",
        "            final_score_list.extend(score_list)\n",
        "\n",
        "        # Structure lists into either pd.Dataframe or json, depending on json_out val\n",
        "        deploy_out = self.format_output(final_user_list, final_item_list, final_score_list, json_out)\n",
        "\n",
        "        return deploy_out\n",
        "\n",
        "    # Restore the best model and generate predictions for all users and items\n",
        "    def deploy_preds(self, this_chunk = 0, num_chunks = 1, user_id=None, json_out=False):\n",
        "\n",
        "        # Restore best model weights\n",
        "        final_ckpt = self.ckpt_manager.latest_checkpoint\n",
        "        self.ckpt.restore(final_ckpt)\n",
        "\n",
        "        # Init lists used in case more than 1 user predictions requested\n",
        "        final_user_list = []\n",
        "        final_item_list = []\n",
        "        final_score_list = []\n",
        "\n",
        "        if type(user_id) == list:\n",
        "            if 0 in user_id:\n",
        "                deploy_out = self.get_all_user_preds(final_user_list, final_item_list, final_score_list, this_chunk, num_chunks, user_id, json_out)\n",
        "                return deploy_out\n",
        "            # Iterate over users and only include predictions for included users\n",
        "            for this_user_id in user_id:\n",
        "\n",
        "                # Convert ids to indices\n",
        "                this_user_idx = self.dataset.user_to_idx.get(this_user_id)\n",
        "\n",
        "                if this_user_idx is not None:\n",
        "                    # Pass in this user's index and return three lists of same length, to be merged into df\n",
        "                    user_list, item_list, score_list = self.user_preds(user_idx = this_user_idx)\n",
        "                else:\n",
        "                    user_list = [this_user_id]\n",
        "                    item_list = [\"\"]\n",
        "                    score_list = [\"\"]\n",
        "                final_user_list.extend(user_list)\n",
        "                final_item_list.extend(item_list)\n",
        "                final_score_list.extend(score_list)\n",
        "\n",
        "            # Structure lists into either pd.Dataframe or json, depending on json_out val\n",
        "            deploy_out = self.format_output(final_user_list, final_item_list, final_score_list, json_out)\n",
        "\n",
        "            return(deploy_out)\n",
        "\n",
        "        # If no user_id, or list of user_ids, iterate over all users (or partition of users)\n",
        "        else:\n",
        "            deploy_out = self.get_all_user_preds(final_user_list, final_item_list, final_score_list, this_chunk, num_chunks, user_id, json_out)\n",
        "            return deploy_out\n",
        "\n",
        "    # Function to train, test, and save best model checkpoint\n",
        "    def train(self):\n",
        "\n",
        "        # Setting tf random seeds must be done inside each graph\n",
        "        np.random.seed(self.args.random_seed)\n",
        "        tf.random.set_seed(self.args.random_seed)\n",
        "\n",
        "        # Initialize bests, starting with 0/-1 so that first score becomes the 'best' after first iteration\n",
        "        best_epoch = 0\n",
        "        best_val_acc = -1\n",
        "        best_test_acc = -1\n",
        "\n",
        "        # Iterate through epoch until at most max_iters, but can auto-stop if model overfits\n",
        "        for epoch in range(self.args.max_iters):\n",
        "\n",
        "            # Iterate seed\n",
        "            #ith_seed = self.args.random_seed * epoch\n",
        "\n",
        "            # Generate training data\n",
        "            pos_users, pos_feats = self.dataset.generate_pos_train_batch_sp(items_per_user = self.args.num_train_samples)\n",
        "            neg_users, neg_feats = self.dataset.generate_neg_train_batch_sp(items_per_user = self.args.num_train_samples)\n",
        "\n",
        "            # Format training data to create predictions, including sparse features\n",
        "            pos_users, sparse_pos_feats = self.feed_dict(pos_users, pos_feats)\n",
        "            neg_users, sparse_neg_feats = self.feed_dict(neg_users, neg_feats)\n",
        "\n",
        "            # Train model by minimizing lost by modifying the trainable variables\n",
        "            self.opt.minimize(self.loss_fn, self.var_list_fn)\n",
        "\n",
        "            # If this epoch should be an evaluation step, calculate accuracy and check for new best model\n",
        "            if epoch % self.args.eval_freq == 0:\n",
        "\n",
        "                # Check if val_set is being used\n",
        "                if self.args.val_set == 1:\n",
        " \n",
        "                    # Generate list of users and features from val dataset\n",
        "                    pos_users, pos_feats = self.dataset.generate_pos_val_batch_sp()\n",
        "                    neg_users, neg_feats = self.dataset.generate_neg_val_batch_sp(items_per_user = self.args.num_val_samples)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    # Generate list of users and features from test dataset\n",
        "                    pos_users, pos_feats = self.dataset.generate_pos_test_batch_sp()\n",
        "                    neg_users, neg_feats = self.dataset.generate_neg_test_batch_sp(items_per_user = self.args.num_val_samples)\n",
        "\n",
        "                # Format training data to create predictions, including sparse features\n",
        "                pos_users, sparse_pos_feats = self.feed_dict(pos_users, pos_feats)\n",
        "                neg_users, sparse_neg_feats = self.feed_dict(neg_users, neg_feats)\n",
        "\n",
        "                pos_preds = self.model(sparse_pos_feats)\n",
        "\n",
        "                # Get the correct number of pos rows to compare with the neg rows\n",
        "                pos_preds = tf.tile(pos_preds, tf.constant([1, self.args.num_val_samples]))\n",
        "\n",
        "                # Reshape the above to match neg_preds shape\n",
        "                pos_preds = tf.reshape(pos_preds, [-1, 1])\n",
        "\n",
        "                # Model scores random negative samples, multiple per user\n",
        "                neg_preds = self.model(sparse_neg_feats)\n",
        "\n",
        "                # Validation Accuracy - how often the correct answer is predicted over the random one\n",
        "                val_acc = np.mean(tf.dtypes.cast(((pos_preds - neg_preds) > 0), tf.float32))\n",
        "                self.args.logger(\"Epoch: \" + str(epoch) + \", Current val ACC: \" + str(val_acc), 1)\n",
        "\n",
        "                # If val_acc is a new best, save this iteration and check test_val\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_epoch = epoch\n",
        "                    best_val_acc = val_acc\n",
        "                      \n",
        "                    # If using a seperate test set, check the test_acc as well\n",
        "                    if self.args.val_set == 1:\n",
        "\n",
        "                        # Generate list of users and features from test dataset\n",
        "                        pos_users, pos_feats = self.dataset.generate_pos_test_batch_sp()\n",
        "                        neg_users, neg_feats = self.dataset.generate_neg_test_batch_sp(items_per_user = self.args.num_val_samples)\n",
        "\n",
        "                        # Format training data to create predictions, including sparse features\n",
        "                        pos_users, sparse_pos_feats = self.feed_dict(pos_users, pos_feats)\n",
        "                        neg_users, sparse_neg_feats = self.feed_dict(neg_users, neg_feats)\n",
        "\n",
        "                        pos_preds = self.model(sparse_pos_feats)\n",
        "\n",
        "                        # Get the correct number of pos rows to compare with the neg rows\n",
        "                        pos_preds = tf.tile(pos_preds, tf.constant([1, self.args.num_val_samples]))\n",
        "\n",
        "                        # Reshape the above to match neg_preds shape\n",
        "                        pos_preds = tf.reshape(pos_preds, [-1, 1])\n",
        "\n",
        "                        # Model scores random negative samples, multiple per user\n",
        "                        neg_preds = self.model(sparse_neg_feats)\n",
        "\n",
        "                        # Validation Accuracy - how often the correct answer is predicted over the random one\n",
        "                        best_test_acc = np.mean(tf.dtypes.cast(((pos_preds - neg_preds) > 0), tf.float32))\n",
        "                        self.args.logger('New best epoch: ' + str(best_epoch) + ', Test ACC: ' +str(best_test_acc), 1)\n",
        "\n",
        "                    # If option to generate deployment predictions is enabled, save checkpoints\n",
        "                    if self.args.deploy_preds == 1:\n",
        "\n",
        "                        #dynamically create the name of the current checkpoint using epoch\n",
        "                        self.most_recent_checkpoint = 'checkpoint_' + str(epoch)\n",
        "                        self.ckpt_manager.save()\n",
        "\n",
        "                # Early stopping criteria\n",
        "                else:\n",
        "\n",
        "                    # Make sure not quitting too early\n",
        "                    if epoch < self.args.min_epoch:\n",
        "                        pass\n",
        "\n",
        "                    # If no improvement in quit_delta rounds, print final results, create preds\n",
        "                    elif epoch >= (best_epoch + self.args.quit_delta):\n",
        "\n",
        "                        self.args.logger('Early stopping, best epoch: ' + str(best_epoch) + ', Val ACC: ' +\n",
        "                                    str(best_val_acc), 1)\n",
        "\n",
        "                        break\n",
        "        # Completed all iterations through max iters, end train\n",
        "        self.args.logger('Finished, best epoch: ' + str(best_epoch) + ', Val ACC: ' +\n",
        "                                    str(best_val_acc) + ', Test ACC: ' +\n",
        "                                    str(best_test_acc), 1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEBzY-x4OL-3"
      },
      "source": [
        "Create MovieLens Dataset\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsoU--o1v6nq",
        "outputId": "b82b382f-5ffb-43de-9409-5c7c0ef32cd0"
      },
      "source": [
        "MovieLens_dataset = Dataset(args)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First pass\n",
            "\tnum_users = 943\n",
            "\tnum_items = 1682\n",
            "\tdf_shape  = (100000, 4)\n",
            "Collected user counts...\n",
            "Collected item counts...\n",
            "User filtering done...\n",
            "Item filtering done...\n",
            "Second pass\n",
            "\tnum_users = 943\n",
            "\tnum_items = 1473\n",
            "\tdf_shape  = (99723, 4)\n",
            "Constructing datasets...\n",
            "Trying to structure dataset with validation set, this isn't fully supported. \n",
            "Reading user demographics...\n",
            "Reading item demographics...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6lv3Aw3Oshi"
      },
      "source": [
        "Create and Train Model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0O9wVqum97G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a5c826-f4a5-4ede-88a9-d015de494ae5"
      },
      "source": [
        "# Init a recommender\n",
        "MovieLens_PRME = Recommender(MovieLens_dataset, args)\n",
        "\n",
        "# Train model\n",
        "MovieLens_PRME.train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature dimension = 3908x3\n",
            "Epoch: 0, Current val ACC: 0.6065748\n",
            "New best epoch: 0, Test ACC: 0.60774124\n",
            "Epoch: 10, Current val ACC: 0.72725344\n",
            "New best epoch: 10, Test ACC: 0.6902439\n",
            "Epoch: 20, Current val ACC: 0.7437964\n",
            "New best epoch: 20, Test ACC: 0.70265114\n",
            "Epoch: 30, Current val ACC: 0.7525981\n",
            "New best epoch: 30, Test ACC: 0.70795333\n",
            "Epoch: 40, Current val ACC: 0.7609756\n",
            "New best epoch: 40, Test ACC: 0.71686107\n",
            "Epoch: 50, Current val ACC: 0.76277834\n",
            "New best epoch: 50, Test ACC: 0.7271474\n",
            "Epoch: 60, Current val ACC: 0.77433723\n",
            "New best epoch: 60, Test ACC: 0.73775184\n",
            "Epoch: 70, Current val ACC: 0.7954401\n",
            "New best epoch: 70, Test ACC: 0.7562036\n",
            "Epoch: 80, Current val ACC: 0.8049841\n",
            "New best epoch: 80, Test ACC: 0.7681866\n",
            "Epoch: 90, Current val ACC: 0.807105\n",
            "New best epoch: 90, Test ACC: 0.7752916\n",
            "Finished, best epoch: 90, Val ACC: 0.807105, Test ACC: 0.7752916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyqm4CiOO8xv"
      },
      "source": [
        "Save Recommendations \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vcG-W6CO7i1"
      },
      "source": [
        "# Save output preds of model as out_df\n",
        "MovieLens_recommendations_df = MovieLens_PRME.deploy_preds()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfqS4nb0Oo-h"
      },
      "source": [
        "Show Recommendations Preview\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "lKri2LcaGMJu",
        "outputId": "6a307c16-4981-4639-e53e-d9eaaf6b16e1"
      },
      "source": [
        "MovieLens_recommendations_df"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>195</td>\n",
              "      <td>241</td>\n",
              "      <td>52.837547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>195</td>\n",
              "      <td>8</td>\n",
              "      <td>44.249889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>195</td>\n",
              "      <td>14</td>\n",
              "      <td>43.996578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>195</td>\n",
              "      <td>217</td>\n",
              "      <td>43.925175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>195</td>\n",
              "      <td>63</td>\n",
              "      <td>43.837166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94295</th>\n",
              "      <td>872</td>\n",
              "      <td>426</td>\n",
              "      <td>78.377869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94296</th>\n",
              "      <td>872</td>\n",
              "      <td>162</td>\n",
              "      <td>78.374184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94297</th>\n",
              "      <td>872</td>\n",
              "      <td>14</td>\n",
              "      <td>78.362679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94298</th>\n",
              "      <td>872</td>\n",
              "      <td>435</td>\n",
              "      <td>78.359436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94299</th>\n",
              "      <td>872</td>\n",
              "      <td>209</td>\n",
              "      <td>78.357643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>94300 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      user_id item_id      score\n",
              "0         195     241  52.837547\n",
              "1         195       8  44.249889\n",
              "2         195      14  43.996578\n",
              "3         195     217  43.925175\n",
              "4         195      63  43.837166\n",
              "...       ...     ...        ...\n",
              "94295     872     426  78.377869\n",
              "94296     872     162  78.374184\n",
              "94297     872      14  78.362679\n",
              "94298     872     435  78.359436\n",
              "94299     872     209  78.357643\n",
              "\n",
              "[94300 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYb_MRl1KdIX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}